#!/usr/bin/env python3
"""
ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³ä¾¡æ ¼ã®ä¸Š/ä¸‹/æ¨ªã°ã„ï¼ˆ3ã‚¯ãƒ©ã‚¹ï¼‰ã‚’äºˆæ¸¬ã™ã‚‹æ™‚ç³»åˆ—åˆ†é¡ãƒ¢ãƒ‡ãƒ«
PyTorch + Transformer Encoder ã‚’ä½¿ç”¨

åˆå¿ƒè€…å‘ã‘ã«è©³ç´°ã‚³ãƒ¡ãƒ³ãƒˆä»˜ã
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import yfinance as yf
import math
import warnings

from utils import get_device
warnings.filterwarnings('ignore')

# ===== 1. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š =====
# ã“ã“ã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ã§æ§˜ã€…ãªè¨­å®šã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã¾ã™
H = 4           # äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ï¼ˆä½•æœ¬å¾Œã‚’äºˆæ¸¬ã™ã‚‹ã‹ï¼‰
L = 256         # å…¥åŠ›ç³»åˆ—é•·ï¼ˆä½•æœ¬åˆ†ã®å±¥æ­´ã‚’è¦‹ã‚‹ã‹ï¼‰
thr = 0.004     # ä¸Šæ˜‡/ä¸‹é™ã‚’åˆ¤å®šã™ã‚‹é–¾å€¤ï¼ˆ0.4%ï¼‰
d_model = 128   # Transformerã®éš ã‚Œå±¤æ¬¡å…ƒæ•°
nhead = 8       # Multi-Head Attentionã®ãƒ˜ãƒƒãƒ‰æ•°
num_layers = 4  # Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
dropout = 0.1   # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
lr = 0.001      # å­¦ç¿’ç‡
batch_size = 64 # ãƒãƒƒãƒã‚µã‚¤ã‚º
max_epochs = 100
patience = 10   # æ—©æœŸçµ‚äº†ã®æˆ‘æ…¢å›æ•°

# ===== 2. yfinanceã‚’ä½¿ç”¨ã—ãŸãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿å–å¾— =====
def get_btc_data(period="2y", interval="1h"):
    """
    yfinanceã‚’ä½¿ã£ã¦ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³ã®OHLCVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

    Args:
        period: ãƒ‡ãƒ¼ã‚¿å–å¾—æœŸé–“ ("1d", "5d", "1mo", "3mo", "6mo", "1y", "2y", "5y", "10y", "ytd", "max")
        interval: æ™‚é–“è¶³ ("1m", "2m", "5m", "15m", "30m", "1h", "1d", "5d", "1wk", "1mo", "3mo")
    """
    print(f"ğŸ“Š yfinanceã‹ã‚‰BTCãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­... (æœŸé–“: {period}, é–“éš”: {interval})")

    try:
        # ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³ã®ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã‚·ãƒ³ãƒœãƒ«
        btc = yf.Ticker("BTC-USD")

        # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        df = btc.history(period=period, interval=interval)

        if df.empty:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

        # ã‚«ãƒ©ãƒ åã‚’çµ±ä¸€
        df.reset_index(inplace=True)
        df.columns = ['timestamp', 'Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']

        # ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤
        df = df[['timestamp', 'Open', 'High', 'Low', 'Close', 'Volume']]

        print(f"âœ… {len(df)}ä»¶ã®BTCãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†")
        print(f"ğŸ“… ãƒ‡ãƒ¼ã‚¿æœŸé–“: {df['timestamp'].min()} ï½ {df['timestamp'].max()}")

        return df

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        print("ğŸ“Š ä»£æ›¿ã¨ã—ã¦ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã™...")
        return generate_dummy_btc_data()

def generate_dummy_btc_data(n_samples=10000, start_price=50000):
    """
    ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³ã®OHLCVãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã®ãƒ†ã‚¹ãƒˆç”¨
    """
    print("ğŸ“Š ãƒ€ãƒŸãƒ¼BTCãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")

    # æ™‚é–“è»¸ã‚’ä½œæˆï¼ˆ15åˆ†è¶³ã‚’æƒ³å®šï¼‰
    dates = pd.date_range(start='2020-01-01', periods=n_samples, freq='15min')

    # ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯çš„ãªä¾¡æ ¼å¤‰å‹•ã‚’ç”Ÿæˆ
    np.random.seed(42)
    log_returns = np.random.normal(0, 0.02, n_samples)  # 2%ã®æ¨™æº–åå·®
    log_returns[0] = 0  # åˆæœŸå€¤

    # ç´¯ç©ãƒªã‚¿ãƒ¼ãƒ³ã‹ã‚‰ä¾¡æ ¼ã‚’ç”Ÿæˆ
    log_prices = np.log(start_price) + np.cumsum(log_returns)
    close_prices = np.exp(log_prices)

    # OHLCã‚’ç”Ÿæˆï¼ˆCloseã‚’åŸºæº–ã«é©å½“ãªå¤‰å‹•ã‚’ä»˜ã‘ã‚‹ï¼‰
    high_mult = np.random.uniform(1.0, 1.03, n_samples)  # High ã¯ Close ã® 0~3% ä¸Š
    low_mult = np.random.uniform(0.97, 1.0, n_samples)   # Low ã¯ Close ã® 0~3% ä¸‹
    open_prices = np.roll(close_prices, 1)  # Open ã¯å‰ã®Close
    open_prices[0] = start_price

    high_prices = close_prices * high_mult
    low_prices = close_prices * low_mult

    # Volumeã‚’ãƒ©ãƒ³ãƒ€ãƒ ç”Ÿæˆ
    base_volume = 1000000
    volumes = np.random.lognormal(np.log(base_volume), 0.5, n_samples)

    df = pd.DataFrame({
        'timestamp': dates,
        'Open': open_prices,
        'High': high_prices,
        'Low': low_prices,
        'Close': close_prices,
        'Volume': volumes
    })

    print(f"âœ… {n_samples}ä»¶ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†")
    return df

# ===== 3. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° =====
def create_features(df):
    """
    OHLCVã‹ã‚‰æ©Ÿæ¢°å­¦ç¿’ç”¨ã®ç‰¹å¾´é‡ã‚’ä½œæˆ
    ä»•æ§˜ã§æŒ‡å®šã•ã‚ŒãŸ5ã¤ã®ç‰¹å¾´é‡ã‚’å®Ÿè£…
    """
    print("ğŸ”§ ç‰¹å¾´é‡ã‚’ä½œæˆä¸­...")

    # DataFrameã‚’ã‚³ãƒ”ãƒ¼ã—ã¦å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿è­·
    data = df.copy()

    # 1. log_return: å¯¾æ•°ãƒªã‚¿ãƒ¼ãƒ³ï¼ˆä¾¡æ ¼å¤‰å‹•ç‡ï¼‰
    data['log_return'] = np.log(data['Close'] / data['Close'].shift(1))

    # 2. hl_range: é«˜å€¤å®‰å€¤ã®ãƒ¬ãƒ³ã‚¸ï¼ˆãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æŒ‡æ¨™ï¼‰
    data['hl_range'] = (data['High'] - data['Low']) / data['Close']

    # 3. close_pos: é«˜å€¤å®‰å€¤ç¯„å›²ã§ã®çµ‚å€¤ä½ç½®ï¼ˆ0=å®‰å€¤ã€1=é«˜å€¤ï¼‰
    data['close_pos'] = (data['Close'] - data['Low']) / (data['High'] - data['Low'] + 1e-9)

    # 4. vol_chg: å‡ºæ¥é«˜å¤‰åŒ–ç‡ï¼ˆ20æœŸé–“ç§»å‹•å¹³å‡ã¨ã®æ¯”è¼ƒï¼‰
    vol_ma20 = data['Volume'].rolling(20).mean()
    data['vol_chg'] = data['Volume'] / vol_ma20 - 1

    # 5. ma20_diff: 20æœŸé–“ç§»å‹•å¹³å‡ã‹ã‚‰ã®ä¹–é›¢ç‡
    close_ma20 = data['Close'].rolling(20).mean()
    data['ma20_diff'] = data['Close'] / close_ma20 - 1

    # NaNï¼ˆæ¬ æå€¤ï¼‰ã‚’é™¤å»ï¼ˆãƒ­ãƒ¼ãƒªãƒ³ã‚°è¨ˆç®—ã§æœ€åˆã®20æœŸé–“ãŒNaNï¼‰
    data = data.dropna()

    print(f"âœ… ç‰¹å¾´é‡ä½œæˆå®Œäº†ã€‚ãƒ‡ãƒ¼ã‚¿æ•°: {len(data)}")
    print(f"ğŸ“ˆ ç‰¹å¾´é‡: {['log_return', 'hl_range', 'close_pos', 'vol_chg', 'ma20_diff']}")

    return data

# ===== 4. ãƒ©ãƒ™ãƒ«ç”Ÿæˆ =====
def create_labels(df, horizon=H, threshold=thr):
    """
    å°†æ¥ã®ä¾¡æ ¼å¤‰å‹•ã‹ã‚‰3ã‚¯ãƒ©ã‚¹ã®ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆ
    0: up (ä¸Šæ˜‡), 1: down (ä¸‹é™), 2: flat (æ¨ªã°ã„)
    """
    print(f"ğŸ·ï¸  ãƒ©ãƒ™ãƒ«ç”Ÿæˆä¸­... (ãƒ›ãƒ©ã‚¤ã‚ºãƒ³={horizon}, é–¾å€¤={threshold:.1%})")

    data = df.copy()

    # Hæœ¬å¾Œã®ä¾¡æ ¼å¤‰å‹•ç‡ã‚’è¨ˆç®—
    future_return = (data['Close'].shift(-horizon) - data['Close']) / data['Close']

    # 3ã‚¯ãƒ©ã‚¹ã«åˆ†é¡
    labels = np.full(len(data), -1, dtype=int)  # åˆæœŸå€¤ã¯-1ï¼ˆç„¡åŠ¹ï¼‰
    labels[future_return >= threshold] = 0      # up
    labels[future_return <= -threshold] = 1     # down
    labels[(future_return > -threshold) & (future_return < threshold)] = 2  # flat

    # å°†æ¥ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ãˆãªã„ã‚µãƒ³ãƒ—ãƒ«ã¯é™¤å¤–
    valid_mask = ~pd.isna(future_return)

    # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    valid_labels = labels[valid_mask]
    up_count = np.sum(valid_labels == 0)
    down_count = np.sum(valid_labels == 1)
    flat_count = np.sum(valid_labels == 2)
    total = len(valid_labels)

    print(f"ğŸ“Š ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ:")
    print(f"   Up (0):   {up_count:6d} ({up_count/total:.1%})")
    print(f"   Down (1): {down_count:6d} ({down_count/total:.1%})")
    print(f"   Flat (2): {flat_count:6d} ({flat_count/total:.1%})")

    return labels, valid_mask

# ===== 5. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹ =====
class BtcSequenceDataset(Dataset):
    """
    æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åŒ–ã™ã‚‹ãŸã‚ã®PyTorchãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    """
    def __init__(self, features, labels, sequence_length=L):
        """
        Args:
            features: [N, F] ã®ç‰¹å¾´é‡é…åˆ—
            labels: [N] ã®ãƒ©ãƒ™ãƒ«é…åˆ—
            sequence_length: å…¥åŠ›ç³»åˆ—é•·
        """
        self.features = features
        self.labels = labels
        self.seq_len = sequence_length

        # æœ‰åŠ¹ãªã‚µãƒ³ãƒ—ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆç³»åˆ—é•·åˆ†ã®å±¥æ­´ãŒã‚ã‚‹ã‚‚ã®ï¼‰
        self.valid_indices = list(range(sequence_length, len(features)))

    def __len__(self):
        return len(self.valid_indices)

    def __getitem__(self, idx):
        # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        data_idx = self.valid_indices[idx]

        # éå»Læœ¬åˆ†ã®ç‰¹å¾´é‡ã‚’å–å¾—
        start_idx = data_idx - self.seq_len
        end_idx = data_idx

        X = self.features[start_idx:end_idx]  # [L, F]
        y = self.labels[data_idx]            # scalar

        return torch.FloatTensor(X), torch.LongTensor([y])

# ===== 6. Transformerãƒ¢ãƒ‡ãƒ« =====
class PositionalEncoding(nn.Module):
    """
    ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ä½ç½®æƒ…å ±ã‚’åŸ‹ã‚è¾¼ã¿ï¼‰
    """
    def __init__(self, d_model, max_length=5000):
        super().__init__()

        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¨ˆç®—
        pe = torch.zeros(max_length, d_model)
        position = torch.arange(0, max_length).unsqueeze(1).float()

        # sin/cosã®å‘¨æœŸçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                           -(math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°æ¬¡å…ƒ
        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°æ¬¡å…ƒ

        self.register_buffer('pe', pe.unsqueeze(0))  # [1, max_length, d_model]

    def forward(self, x):
        # x: [batch, seq_len, d_model]
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len]

class BtcClassifier(nn.Module):
    """
    Transformer Encoderãƒ™ãƒ¼ã‚¹ã®ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³ä¾¡æ ¼åˆ†é¡ãƒ¢ãƒ‡ãƒ«
    """
    def __init__(self, input_dim, d_model=128, nhead=8, num_layers=4, dropout=0.1):
        super().__init__()

        # å…¥åŠ›ç‰¹å¾´é‡ã‚’d_modelæ¬¡å…ƒã«å¤‰æ›
        self.input_projection = nn.Linear(input_dim, d_model)

        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        self.pos_encoding = PositionalEncoding(d_model)

        # Transformer Encoderå±¤
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,  # FFNå±¤ã¯é€šå¸¸4å€ã®æ¬¡å…ƒ
            dropout=dropout,
            batch_first=True  # ãƒãƒƒãƒæ¬¡å…ƒã‚’æœ€åˆã«
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼ˆå¹³å‡poolingï¼‰
        self.pool = nn.AdaptiveAvgPool1d(1)

        # åˆ†é¡ãƒ˜ãƒƒãƒ‰
        self.classifier = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, 3)  # 3ã‚¯ãƒ©ã‚¹åˆ†é¡
        )

    def forward(self, x):
        # x: [batch, seq_len, input_dim]

        # å…¥åŠ›ã‚’åŸ‹ã‚è¾¼ã¿å±¤ã«é€šã™
        x = self.input_projection(x)  # [batch, seq_len, d_model]

        # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿½åŠ 
        x = self.pos_encoding(x)

        # Transformerã§æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
        x = self.transformer(x)  # [batch, seq_len, d_model]

        # ç³»åˆ—å…¨ä½“ã‚’1ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«ã«é›†ç´„ï¼ˆå¹³å‡poolingï¼‰
        x = x.transpose(1, 2)  # [batch, d_model, seq_len]
        x = self.pool(x)       # [batch, d_model, 1]
        x = x.squeeze(-1)      # [batch, d_model]

        # åˆ†é¡
        logits = self.classifier(x)  # [batch, 3]

        return logits

# ===== 7. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã¨å‰å‡¦ç† =====
def prepare_data(df):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆã«åˆ†å‰²ã—ã€æ­£è¦åŒ–ã‚’é©ç”¨
    """
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ãƒ»æ­£è¦åŒ–ä¸­...")

    # ç‰¹å¾´é‡åˆ—ã‚’å–å¾—
    feature_cols = ['log_return', 'hl_range', 'close_pos', 'vol_chg', 'ma20_diff']
    features = df[feature_cols].values  # [N, F]

    # ãƒ©ãƒ™ãƒ«ç”Ÿæˆ
    labels, valid_mask = create_labels(df)

    # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨
    features = features[valid_mask]
    labels = labels[valid_mask]

    # æ™‚ç³»åˆ—é †ã«åˆ†å‰²ï¼ˆãƒªãƒ¼ã‚¯ã‚’é˜²ããŸã‚ï¼‰
    n_total = len(features)
    n_train = int(0.7 * n_total)
    n_val = int(0.15 * n_total)

    # åˆ†å‰²
    X_train = features[:n_train]
    X_val = features[n_train:n_train+n_val]
    X_test = features[n_train+n_val:]

    y_train = labels[:n_train]
    y_val = labels[n_train:n_train+n_val]
    y_test = labels[n_train+n_val:]

    # æ­£è¦åŒ–ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆé‡ã§fit â†’ å…¨ãƒ‡ãƒ¼ã‚¿ã«é©ç”¨ï¼‰
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)

    print(f"âœ… ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†:")
    print(f"   è¨“ç·´: {len(X_train)} ã‚µãƒ³ãƒ—ãƒ«")
    print(f"   æ¤œè¨¼: {len(X_val)} ã‚µãƒ³ãƒ—ãƒ«")
    print(f"   ãƒ†ã‚¹ãƒˆ: {len(X_test)} ã‚µãƒ³ãƒ—ãƒ«")

    return (X_train_scaled, X_val_scaled, X_test_scaled,
            y_train, y_val, y_test, scaler)

# ===== 8. å­¦ç¿’ãƒ«ãƒ¼ãƒ— =====
def train_model(model, train_loader, val_loader, num_epochs=max_epochs, patience=patience):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€æ—©æœŸçµ‚äº†ã¨ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç®¡ç†
    """
    print(f"ğŸš€ å­¦ç¿’é–‹å§‹ (æœ€å¤§{num_epochs}ã‚¨ãƒãƒƒã‚¯, æ—©æœŸçµ‚äº†patience={patience})")

    # æå¤±é–¢æ•°ã¨æœ€é©åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)

    # æ—©æœŸçµ‚äº†ç”¨ã®å¤‰æ•°
    best_val_loss = float('inf')
    patience_counter = 0
    best_model_state = None

    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        # === è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚º ===
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        for batch_X, batch_y in train_loader:
            # ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•ï¼ˆGPUä½¿ç”¨æ™‚ï¼‰
            device = next(model.parameters()).device
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            batch_y = batch_y.squeeze()

            # é †ä¼æ’­
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)

            # é€†ä¼æ’­
            loss.backward()

            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå‹¾é…çˆ†ç™ºé˜²æ­¢ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            # çµ±è¨ˆæ›´æ–°
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += batch_y.size(0)
            train_correct += predicted.eq(batch_y).sum().item()

        # === æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º ===
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                batch_y = batch_y.squeeze()

                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)

                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += batch_y.size(0)
                val_correct += predicted.eq(batch_y).sum().item()

        # å¹³å‡æå¤±ãƒ»ç²¾åº¦è¨ˆç®—
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total

        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)

        # ãƒ­ã‚°å‡ºåŠ›
        print(f"ã‚¨ãƒãƒƒã‚¯ {epoch+1:3d}: "
              f"Train Loss: {avg_train_loss:.4f} ({train_acc:.1f}%) | "
              f"Val Loss: {avg_val_loss:.4f} ({val_acc:.1f}%)")

        # æ—©æœŸçµ‚äº†ã®åˆ¤å®š
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            best_model_state = model.state_dict().copy()  # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
            print(f"ğŸ“ˆ æ–°ã—ã„ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ« (Val Loss: {best_val_loss:.4f})")
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f"â° æ—©æœŸçµ‚äº†: {patience}ã‚¨ãƒãƒƒã‚¯æ”¹å–„ãªã—")
            break

    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’å¾©å…ƒ
    model.load_state_dict(best_model_state)
    print(f"âœ… å­¦ç¿’å®Œäº†! ãƒ™ã‚¹ãƒˆVal Loss: {best_val_loss:.4f}")

    return train_losses, val_losses

# ===== 9. è©•ä¾¡é–¢æ•° =====
def evaluate_model(model, test_loader):
    """
    ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’è©•ä¾¡
    """
    print("ğŸ” ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ä¸­...")

    model.eval()
    all_predictions = []
    all_targets = []

    device = next(model.parameters()).device

    with torch.no_grad():
        for batch_X, batch_y in test_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            batch_y = batch_y.squeeze()

            outputs = model(batch_X)
            _, predicted = outputs.max(1)

            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(batch_y.cpu().numpy())

    # è©³ç´°ãªåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
    class_names = ['Up', 'Down', 'Flat']
    print("\nğŸ“Š åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
    print(classification_report(all_targets, all_predictions, target_names=class_names))

    print("\nğŸ”„ æ··åŒè¡Œåˆ—:")
    cm = confusion_matrix(all_targets, all_predictions)
    print("      ", "  ".join([f"{name:>6}" for name in class_names]))
    for i, (true_name, row) in enumerate(zip(class_names, cm)):
        print(f"{true_name:>4}: {' '.join([f'{val:6d}' for val in row])}")

    return all_predictions, all_targets

# ===== 10. æ¨è«–é–¢æ•° =====
def predict_proba(model, scaler, features_sequence):
    """
    1ã¤ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã—ã¦äºˆæ¸¬ç¢ºç‡ã‚’è¿”ã™

    Args:
        model: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
        scaler: å­¦ç¿’æ™‚ã«ä½¿ã£ãŸã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼
        features_sequence: [L, F] ã®ç‰¹å¾´é‡ç³»åˆ—

    Returns:
        dict: {"p_up": float, "p_down": float, "p_flat": float}
    """
    model.eval()
    device = next(model.parameters()).device

    # æ­£è¦åŒ–
    features_scaled = scaler.transform(features_sequence.reshape(-1, features_sequence.shape[-1]))
    features_scaled = features_scaled.reshape(features_sequence.shape)

    # ãƒ†ãƒ³ã‚½ãƒ«åŒ–ã—ã¦ãƒãƒƒãƒæ¬¡å…ƒè¿½åŠ 
    X = torch.FloatTensor(features_scaled).unsqueeze(0).to(device)  # [1, L, F]

    with torch.no_grad():
        logits = model(X)  # [1, 3]
        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]  # [3]

    return {
        "p_up": float(probs[0]),
        "p_down": float(probs[1]),
        "p_flat": float(probs[2])
    }

# ===== 11. ç°¡æ˜“ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ =====
def simple_backtest(model, scaler, X_test, y_test, df_test_period):
    """
    ç°¡å˜ãªå–å¼•ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    print("ğŸ’° ç°¡æ˜“ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")

    device = next(model.parameters()).device

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹äºˆæ¸¬
    test_dataset = BtcSequenceDataset(X_test, y_test, L)

    trades = []

    for i in range(len(test_dataset)):
        X_seq, y_true = test_dataset[i]

        # äºˆæ¸¬
        X_batch = X_seq.unsqueeze(0).to(device)
        with torch.no_grad():
            logits = model(X_batch)
            probs = torch.softmax(logits, dim=1).cpu().numpy()[0]

        p_up, p_down, p_flat = probs[0], probs[1], probs[2]

        # ãƒˆãƒ¬ãƒ¼ãƒ‰åˆ¤å®š
        conf = max(p_up, p_down)
        edge = p_up - p_down

        action = 'hold'
        if conf >= 0.55 and edge >= 0.10:
            action = 'long'
        elif conf >= 0.55 and edge <= -0.10:
            action = 'short'

        trades.append({
            'action': action,
            'confidence': conf,
            'edge': edge,
            'actual': y_true.item(),
            'p_up': p_up,
            'p_down': p_down,
            'p_flat': p_flat
        })

    # æˆç¸¾é›†è¨ˆ
    total_trades = len([t for t in trades if t['action'] != 'hold'])
    long_trades = [t for t in trades if t['action'] == 'long']
    short_trades = [t for t in trades if t['action'] == 'short']

    # æ‰‹æ•°æ–™
    fee_rate = 0.0004  # 0.04%

    total_pnl = 0
    correct_trades = 0

    for trade in trades:
        if trade['action'] == 'long':
            # ãƒ­ãƒ³ã‚°ãŒæˆåŠŸ = å®Ÿéš›ã«ä¸Šæ˜‡
            if trade['actual'] == 0:  # up
                pnl = thr - fee_rate  # åˆ©ç›Š - æ‰‹æ•°æ–™
                correct_trades += 1
            else:
                pnl = -thr - fee_rate  # æå¤± - æ‰‹æ•°æ–™
            total_pnl += pnl

        elif trade['action'] == 'short':
            # ã‚·ãƒ§ãƒ¼ãƒˆãŒæˆåŠŸ = å®Ÿéš›ã«ä¸‹é™
            if trade['actual'] == 1:  # down
                pnl = thr - fee_rate
                correct_trades += 1
            else:
                pnl = -thr - fee_rate
            total_pnl += pnl

    win_rate = correct_trades / total_trades if total_trades > 0 else 0
    avg_pnl = total_pnl / total_trades if total_trades > 0 else 0

    print(f"\nğŸ’¼ ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆçµæœ:")
    print(f"   ç·å–å¼•æ•°: {total_trades}")
    print(f"   ãƒ­ãƒ³ã‚°: {len(long_trades)}, ã‚·ãƒ§ãƒ¼ãƒˆ: {len(short_trades)}")
    print(f"   å‹ç‡: {win_rate:.1%}")
    print(f"   ç·æç›Š: {total_pnl:.1%}")
    print(f"   å¹³å‡æç›Š: {avg_pnl:.3%}")
    print(f"   æ‰‹æ•°æ–™è€ƒæ…®æ¸ˆã¿ (ç‰‡é“{fee_rate:.2%})")

# ===== 12. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•° =====
def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    print("ğŸš€ ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³ä¾¡æ ¼åˆ†é¡ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œé–‹å§‹!")
    print("=" * 60)

    # Step 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆyfinanceã‹ã‚‰å®Ÿéš›ã®BTCãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼‰
    df = get_btc_data(period="2y", interval="1h")

    # Step 2: ç‰¹å¾´é‡ä½œæˆ
    df_with_features = create_features(df)

    # Step 3: ãƒ‡ãƒ¼ã‚¿æº–å‚™
    X_train, X_val, X_test, y_train, y_val, y_test, scaler = prepare_data(df_with_features)

    # Step 4: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
    train_dataset = BtcSequenceDataset(X_train, y_train, L)
    val_dataset = BtcSequenceDataset(X_val, y_val, L)
    test_dataset = BtcSequenceDataset(X_test, y_test, L)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # Step 5: ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    input_dim = X_train.shape[1]  # ç‰¹å¾´é‡æ•°
    model = BtcClassifier(input_dim, d_model, nhead, num_layers, dropout)

    # çµ±ä¸€åŒ–ã•ã‚ŒãŸãƒ‡ãƒã‚¤ã‚¹å–å¾—
    device = get_device()
    model = model.to(device)
    print(f"ğŸ”§ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    print(f"ğŸ—ï¸  ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")

    # Step 6: å­¦ç¿’
    train_losses, val_losses = train_model(model, train_loader, val_loader)

    # Step 7: è©•ä¾¡
    predictions, targets = evaluate_model(model, test_loader)

    # Step 8: ã‚µãƒ³ãƒ—ãƒ«æ¨è«–
    print("\nğŸ”® ã‚µãƒ³ãƒ—ãƒ«æ¨è«–:")
    sample_features = X_test[-L:]  # æœ€å¾Œã®Lå€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ç”¨
    result = predict_proba(model, scaler, sample_features)
    print(f"   äºˆæ¸¬ç¢ºç‡: Up={result['p_up']:.3f}, Down={result['p_down']:.3f}, Flat={result['p_flat']:.3f}")

    # Step 9: ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
    simple_backtest(model, scaler, X_test, y_test, df_with_features.iloc[-len(X_test):])

    print("\n" + "=" * 60)
    print("âœ… å…¨å‡¦ç†å®Œäº†!")

    return model, scaler

# ===== è¨­å®šå¤‰æ›´ã‚¬ã‚¤ãƒ‰ =====
"""
ğŸ”§ è¨­å®šå¤‰æ›´ã‚¬ã‚¤ãƒ‰:

1. æ™‚é–“è¶³ã‚’å¤‰æ›´ã—ãŸã„å ´åˆ:
   - generate_dummy_btc_data() é–¢æ•°ã® freq='15min' ã‚’å¤‰æ›´
   - å®Ÿãƒ‡ãƒ¼ã‚¿ä½¿ç”¨æ™‚ã¯ã€ãƒ‡ãƒ¼ã‚¿å–å¾—æ™‚ã®æ™‚é–“è¶³ã‚’æŒ‡å®š

2. äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ï¼ˆä½•æœ¬å…ˆã‚’äºˆæ¸¬ã™ã‚‹ã‹ï¼‰ã‚’å¤‰æ›´:
   - å†’é ­ã® H = 4 ã‚’å¤‰æ›´

3. é–¾å€¤ï¼ˆä¸Šæ˜‡/ä¸‹é™åˆ¤å®šï¼‰ã‚’å¤‰æ›´:
   - å†’é ­ã® thr = 0.004 ã‚’å¤‰æ›´

4. ãƒ¢ãƒ‡ãƒ«ã®è¤‡é›‘ã•ã‚’å¤‰æ›´:
   - d_model: Transformerã®éš ã‚Œå±¤æ¬¡å…ƒï¼ˆå¤§ããã™ã‚‹ã»ã©è¤‡é›‘ï¼‰
   - num_layers: Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ï¼ˆæ·±ãã™ã‚‹ã»ã©è¤‡é›‘ï¼‰
   - nhead: Attentionãƒ˜ãƒƒãƒ‰æ•°

5. å­¦ç¿’ç³»åˆ—é•·ã‚’å¤‰æ›´:
   - L = 256 ã‚’å¤‰æ›´ï¼ˆä½•æœ¬åˆ†ã®å±¥æ­´ã‚’è¦‹ã‚‹ã‹ï¼‰

6. å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´:
   - lr: å­¦ç¿’ç‡
   - batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
   - max_epochs: æœ€å¤§ã‚¨ãƒãƒƒã‚¯æ•°
   - patience: æ—©æœŸçµ‚äº†ã®æˆ‘æ…¢å›æ•°

7. ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ãŸã„å ´åˆ:
   - create_features() é–¢æ•°ã§æ–°ã—ã„ç‰¹å¾´é‡ã‚’è¨ˆç®—
   - feature_cols ãƒªã‚¹ãƒˆã«åˆ—åã‚’è¿½åŠ 

8. ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã®æ¡ä»¶å¤‰æ›´:
   - simple_backtest() é–¢æ•°å†…ã® conf >= 0.55 ã‚„ edge >= 0.10 ã®é–¾å€¤
   - fee_rate ã®æ‰‹æ•°æ–™ç‡

å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ:
   - generate_dummy_btc_data() ã®ä»£ã‚ã‚Šã«ã€yfinance ã‚„ ccxt ãªã©ã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿å–å¾—
   - df ã®åˆ—åã¯ 'Open', 'High', 'Low', 'Close', 'Volume' ã§çµ±ä¸€
"""

if __name__ == "__main__":
    model, scaler = main()