#!/usr/bin/env python3
"""
ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from sklearn.metrics import classification_report, confusion_matrix
import pickle
from pathlib import Path

# è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from btc_data import get_btc_data, create_features, prepare_data, BtcSequenceDataset
from btc_model import BtcClassifier
from utils import get_device

# ===== è¨­å®š =====
# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
H = 4           # äºˆæ¸¬ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ï¼ˆä½•æœ¬å¾Œã‚’äºˆæ¸¬ã™ã‚‹ã‹ï¼‰
L = 256         # å…¥åŠ›ç³»åˆ—é•·ï¼ˆä½•æœ¬åˆ†ã®å±¥æ­´ã‚’è¦‹ã‚‹ã‹ï¼‰
thr = 0.004     # ä¸Šæ˜‡/ä¸‹é™ã‚’åˆ¤å®šã™ã‚‹é–¾å€¤ï¼ˆ0.4%ï¼‰
d_model = 128   # Transformerã®éš ã‚Œå±¤æ¬¡å…ƒæ•°
nhead = 8       # Multi-Head Attentionã®ãƒ˜ãƒƒãƒ‰æ•°
num_layers = 4  # Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°
dropout = 0.1   # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
lr = 0.001      # å­¦ç¿’ç‡
batch_size = 64 # ãƒãƒƒãƒã‚µã‚¤ã‚º
max_epochs = 100
patience = 10   # æ—©æœŸçµ‚äº†ã®æˆ‘æ…¢å›æ•°

# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
CHECKPOINT_DIR = Path("checkpoints/btc_classifier")
CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)

MODEL_PATH = CHECKPOINT_DIR / "model.pt"
SCALER_PATH = CHECKPOINT_DIR / "scaler.pkl"
CONFIG_PATH = CHECKPOINT_DIR / "config.pkl"

# ===== å­¦ç¿’ãƒ«ãƒ¼ãƒ— =====
def train_model(model, train_loader, val_loader, num_epochs=max_epochs, patience=patience):
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€æ—©æœŸçµ‚äº†ã¨ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç®¡ç†
    """
    print(f"ğŸš€ å­¦ç¿’é–‹å§‹ (æœ€å¤§{num_epochs}ã‚¨ãƒãƒƒã‚¯, æ—©æœŸçµ‚äº†patience={patience})")

    # æå¤±é–¢æ•°ã¨æœ€é©åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)

    # æ—©æœŸçµ‚äº†ç”¨ã®å¤‰æ•°
    best_val_loss = float('inf')
    patience_counter = 0
    best_model_state = None

    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        # === è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚º ===
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        for batch_X, batch_y in train_loader:
            # ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•ï¼ˆGPUä½¿ç”¨æ™‚ï¼‰
            device = next(model.parameters()).device
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            batch_y = batch_y.squeeze()

            # é †ä¼æ’­
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)

            # é€†ä¼æ’­
            loss.backward()

            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå‹¾é…çˆ†ç™ºé˜²æ­¢ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            # çµ±è¨ˆæ›´æ–°
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            train_total += batch_y.size(0)
            train_correct += predicted.eq(batch_y).sum().item()

        # === æ¤œè¨¼ãƒ•ã‚§ãƒ¼ã‚º ===
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                batch_y = batch_y.squeeze()

                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)

                val_loss += loss.item()
                _, predicted = outputs.max(1)
                val_total += batch_y.size(0)
                val_correct += predicted.eq(batch_y).sum().item()

        # å¹³å‡æå¤±ãƒ»ç²¾åº¦è¨ˆç®—
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total

        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)

        # ãƒ­ã‚°å‡ºåŠ›
        print(f"ã‚¨ãƒãƒƒã‚¯ {epoch+1:3d}: "
              f"Train Loss: {avg_train_loss:.4f} ({train_acc:.1f}%) | "
              f"Val Loss: {avg_val_loss:.4f} ({val_acc:.1f}%)")

        # æ—©æœŸçµ‚äº†ã®åˆ¤å®š
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            best_model_state = model.state_dict().copy()  # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
            print(f"ğŸ“ˆ æ–°ã—ã„ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ« (Val Loss: {best_val_loss:.4f})")
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f"â° æ—©æœŸçµ‚äº†: {patience}ã‚¨ãƒãƒƒã‚¯æ”¹å–„ãªã—")
            break

    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’å¾©å…ƒ
    model.load_state_dict(best_model_state)
    print(f"âœ… å­¦ç¿’å®Œäº†! ãƒ™ã‚¹ãƒˆVal Loss: {best_val_loss:.4f}")

    return train_losses, val_losses

# ===== è©•ä¾¡é–¢æ•° =====
def evaluate_model(model, test_loader):
    """
    ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’è©•ä¾¡
    """
    print("ğŸ” ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ä¸­...")

    model.eval()
    all_predictions = []
    all_targets = []

    device = next(model.parameters()).device

    with torch.no_grad():
        for batch_X, batch_y in test_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            batch_y = batch_y.squeeze()

            outputs = model(batch_X)
            _, predicted = outputs.max(1)

            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(batch_y.cpu().numpy())

    # è©³ç´°ãªåˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ
    class_names = ['Up', 'Down', 'Flat']
    print("\nğŸ“Š åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
    print(classification_report(all_targets, all_predictions, target_names=class_names))

    print("\nğŸ”„ æ··åŒè¡Œåˆ—:")
    cm = confusion_matrix(all_targets, all_predictions)
    print("      ", "  ".join([f"{name:>6}" for name in class_names]))
    for true_name, row in zip(class_names, cm):
        print(f"{true_name:>4}: {' '.join([f'{val:6d}' for val in row])}")

    return all_predictions, all_targets

# ===== ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ =====
def save_checkpoint(model, scaler, config):
    """
    ãƒ¢ãƒ‡ãƒ«ã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã€è¨­å®šã‚’ä¿å­˜
    """
    print("ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ä¸­...")

    # ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ä¿å­˜
    torch.save(model.state_dict(), MODEL_PATH)

    # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚’ä¿å­˜
    with open(SCALER_PATH, 'wb') as f:
        pickle.dump(scaler, f)

    # è¨­å®šã‚’ä¿å­˜ï¼ˆæ¨è«–æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’å†æ§‹ç¯‰ã™ã‚‹ãŸã‚ï¼‰
    with open(CONFIG_PATH, 'wb') as f:
        pickle.dump(config, f)

    print(f"âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å®Œäº†:")
    print(f"   ãƒ¢ãƒ‡ãƒ«: {MODEL_PATH}")
    print(f"   ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼: {SCALER_PATH}")
    print(f"   è¨­å®š: {CONFIG_PATH}")

# ===== ãƒ¡ã‚¤ãƒ³å­¦ç¿’é–¢æ•° =====
def main():
    """
    ãƒ¡ã‚¤ãƒ³å­¦ç¿’é–¢æ•°
    """
    print("ğŸš€ ãƒ“ãƒƒãƒˆã‚³ã‚¤ãƒ³ä¾¡æ ¼åˆ†é¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹!")
    print("=" * 60)

    # Step 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆyfinanceã‹ã‚‰å®Ÿéš›ã®BTCãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼‰
    df = get_btc_data(period="2y", interval="1h")

    # Step 2: ç‰¹å¾´é‡ä½œæˆ
    df_with_features = create_features(df)

    # Step 3: ãƒ‡ãƒ¼ã‚¿æº–å‚™
    X_train, X_val, X_test, y_train, y_val, y_test, scaler = prepare_data(
        df_with_features, horizon=H, threshold=thr
    )

    # Step 4: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
    train_dataset = BtcSequenceDataset(X_train, y_train, L)
    val_dataset = BtcSequenceDataset(X_val, y_val, L)
    test_dataset = BtcSequenceDataset(X_test, y_test, L)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # Step 5: ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    input_dim = X_train.shape[1]  # ç‰¹å¾´é‡æ•°
    model = BtcClassifier(input_dim, d_model, nhead, num_layers, dropout)

    # çµ±ä¸€åŒ–ã•ã‚ŒãŸãƒ‡ãƒã‚¤ã‚¹å–å¾—
    device = get_device()
    model = model.to(device)
    print(f"ğŸ”§ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    print(f"ğŸ—ï¸  ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")

    # Step 6: å­¦ç¿’
    train_model(model, train_loader, val_loader)

    # Step 7: è©•ä¾¡
    evaluate_model(model, test_loader)

    # Step 8: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
    config = {
        'input_dim': input_dim,
        'd_model': d_model,
        'nhead': nhead,
        'num_layers': num_layers,
        'dropout': dropout,
        'sequence_length': L,
        'horizon': H,
        'threshold': thr,
        'feature_columns': ['log_return', 'hl_range', 'close_pos', 'vol_chg', 'ma20_diff']
    }

    save_checkpoint(model, scaler, config)

    print("\n" + "=" * 60)
    print("âœ… å­¦ç¿’å®Œäº†!")
    print(f"ğŸ“ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ {CHECKPOINT_DIR} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

if __name__ == "__main__":
    main()